<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>Understanding Connections &amp; Pools</title><meta name="description" content="What connections are, how they affect our systems, and how and why pooling works."/><meta name="next-head-count" content="4"/><link rel="preload" href="../_next/static/css/5fc65f991220c427.css" as="style"/><link rel="stylesheet" href="../_next/static/css/5fc65f991220c427.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="../_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="../_next/static/chunks/webpack-514908bffb652963.js" defer=""></script><script src="../_next/static/chunks/framework-6e4ba497ae0c8a3f.js" defer=""></script><script src="../_next/static/chunks/main-4cc32e52ef88f8fd.js" defer=""></script><script src="../_next/static/chunks/pages/_app-859a8f91abab2976.js" defer=""></script><script src="../_next/static/chunks/pages/understanding-connections-pools-7ad7c876954c0426.js" defer=""></script><script src="../_next/static/pkInIokvrXIdaP_t5UMJ1/_buildManifest.js" defer=""></script><script src="../_next/static/pkInIokvrXIdaP_t5UMJ1/_ssgManifest.js" defer=""></script><script src="../_next/static/pkInIokvrXIdaP_t5UMJ1/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="antialiased text-gray-900 px-4 py-10 max-w-3xl mx-auto sm:px-6 sm:py-12 lg:max-w-4xl lg:py-16 lg:px-8 xl:max-w-6xl"><article class="prose"><h1>Understanding Connections &amp; Pools</h1><p class="lead">What connections are, how they affect our systems, and how and why pooling works—with notes on how popular applications servers and databases handle connections.</p><p>Connections are the hidden mechanism using which computer systems talk to each other—and they&#x27;ve become so fundamental that we overlook how important they are, how they work, and when they fail. We&#x27;re often ignorant of them until there&#x27;s a problem, which usually shows up a massive failure when our systems are doing their most amount of work. But because they&#x27;re present everywhere and are so important in pretty much every system, they&#x27;re worth spending a little time understanding.</p><h2>What are connections?</h2><p>Connections are a link between two systems that allows them to exchange information as a sequence of zeroes and ones—to send and receive bytes.</p><p>Depending on where the systems are running relative to each other, a combination of underlying software and hardware will work hard to handle the physical movement of information, which <em>abstracts</em> it away. For example, if the communicating systems are two Unix processes, the <a href="https://www.usna.edu/Users/cs/wcbrown/courses/IC221/classes/L13/Class.html">IPC</a> system will handle allocating memory for the data exchanged and will handle pick-up and delivery of the bytes on both sides. If the systems are running on different computers, they will likely communicate over <a href="https://www.khanacademy.org/computing/computers-and-internet/xcae6f4a7ff015e7d:the-internet/xcae6f4a7ff015e7d:transporting-packets/a/transmission-control-protocol--tcp">TCP</a>, which will handle moving the data over a wired or wireless system between the computers. The details of how computers work together to reliably handle, transmit and receive the data is more a standardization problem, and most systems use the building blocks provided by the UDP and TCP protocols. How these connections are handled at each end is a more relevant problem for application development, which is what we&#x27;ll look at now.</p><h2>Where do we use connections?</h2><p>You&#x27;re using them right now. Your browser made a connection to the web server that&#x27;s hosting this blog, using which it fetched the bytes that make up the HTML, CSS, JavaScript and images that you&#x27;re looking at. If you&#x27;re using the HTTP/1.1 protocol, your browser made multiple connections to the server, one for each file. If you used HTTP/2, many of the files were likely served over the same connection, using <em>multiplexing</em>. In these cases your browser was the <em>client</em> and the blog server was... well, the <em>server</em>.</p><p>But the server also made connections of its own to give you this page. It used a connection to speak to a database, sending over the URL of this page inside a query and receiving the contents of the page in return. In this scenario, the application server was the <em>client</em> and the database server was the <em>server</em>. The application server might have also made connections to other third-party services, like a subscription or payment service, or a location service.</p><p>For static files, like the JS, CSS and images, there&#x27;s a CDN system in between your browser and the blog server. A connection was made from your browser (client) to the CDN server (server) that&#x27;s closest to you, and if the files weren&#x27;t available in the cache near you there would have been another connection from the CDN server (client) to the blog server (server).</p><p>If you think carefully about all the systems you use or build, you&#x27;ll see connections all over the place—but they&#x27;re often hidden from view, and not understanding their invisible presence and limits will come back to bite you when (and where) you least expect it.</p><h2>Why is connection handling important?</h2><p>Understanding how connections are handled is important because the cost of connections is <em>asymmetric</em>—the cost is different on the client and server. In a peer to peer (P2P) system, like a torrent cloud, this is false, and connections have the same cost at both ends—but this is rarely the case. The common uses of connections have a client and a server, and the cost to the server is different from the cost to the client.</p><p>Before we look at how connections can be handled, we need to quickly review the different ways in which computers run programs and how programs do work in parallel. When you run a program, the operating system runs your code as one instance of a <em>process</em>. A <em>process</em> occupies one CPU core and some memory when it&#x27;s running, and does not share its memory with any other processes. The process can start <em>threads</em>, which are like children of the process that can run concurrently<em>. Threads</em> share memory with the process that spawned them, and might allocate more memory for their use. Or the process might use an <em>event loop</em>, which is a single-processes system that keeps track of tasks it has to do and loops over all its tasks continuously and infinitely, each time doing the tasks that it can, or skipping them if they&#x27;re blocked. Or the process might use internal constructs called <em>fibers, green-threads, coroutines,</em> or _actors—_each of these work a little differently and having varying costs—but they&#x27;re all managed internally by the process and its threads.</p><p>Coming back to how connections are handled, let&#x27;s look at database connections first—from your application server (the client in this case), you see a TCP connection paid for with a small memory buffer and one port allocation. On the server side, if you&#x27;re using PostgreSQL, each connection is <a href="https://brandur.org/postgres-connections">handled</a> at the server by spawning a new process that handles all the queries being sent over that connection. This process occupies a CPU core, and about 10MB or more of memory in RAM. MySQL <a href="https://mysqlserverteam.com/mysql-connection-handling-and-scaling/">handles</a> each connection by spawning a thread inside of a process. The RAM/memory requirements are much lower in a threaded model, but it comes at the cost of context switching those threads. Redis <a href="https://redis.io/topics/clients">handles</a> each connection as an iteration in an event loop, which makes the resource costs low—but a price is paid in having to loop over every connection&#x27;s queries and serving them strictly one at a time.</p><p>Consider a request to an application server. Your browser initiates a TCP connection as a client, which is cheap (small memory buffer and one port). On the server, the story is different. If the server is using Ruby on Rails, each connection is handled by one thread spawned inside a fixed number of running processes (the <a href="https://puma.io">Puma</a> web server) or by one process (<a href="https://github.com/defunkt/unicorn">Unicorn</a>). If it&#x27;s using PHP, the CGI systems start a new PHP process for each connection, and the more popular <a href="https://en.wikipedia.org/wiki/FastCGI">FastCGI</a> systems keep a few of the processes running to make handling the next connection faster. If you&#x27;re using Go, one <em>goroutine</em> (a cheap and light thread-like structure, managed &amp; scheduled internally by the Go runtime) will be spawned to handle each connection. If you&#x27;re using NodeJS/Deno, the incoming connections are handled in an event loop by iterating over them and responding to requests one at a time. In systems like Erlang/Elixir, each connection will be handled by an <em>actor</em>, which is another internally scheduled lightweight thread-like construct.</p><h2>Connection Handling Architectures</h2><p>The examples of how connections are handled have a few common strategies which we can identify:</p><p><strong>Processes:</strong> Each connection is handled by a separate process, either started exclusively for the connection (CGI, PostgreSQL), or maintained as part of a group of available processes (Unicorn, FastCGI).</p><p><strong>Threads:</strong> Each connection is handled by a separate thread, either spawned exclusively for the connection or held in reserve after spawning. The threads might be spread over multiple processes, but all threads are equivalent (Puma/Ruby, Tomcat/Java, MySQL).</p><p><strong>Event Loop:</strong> Each connection is an task in the event loop, and connections that have data to be read are processed by iterating over them (Node, Redis). These systems are normally single-process and single-threaded, but they may sometimes be multi-process, where each process acts as a semi-independent system with separate event loops.</p><p><strong>Coroutines / Green-Threads / Fibers / Actors:</strong> Each connection is handled by a lightweight construct whose scheduling is managed internally (Go, Erlang, Scala/Akka).</p><p>Knowing how your server is handling connections is crucial to understanding what its limits and scaling patterns are. Even basic usage or configuration requires knowledge of how the connection handling works: Redis and PostgreSQL, for example, offer different transaction &amp; locking semantics that are influenced by their respective connection handling mechanisms. Process &amp; thread based servers can crash because of resource exhaustion if their max counts are not set to a reasonable limit, and when limits are set they might be horribly under-utilized because the limits are too low. Event-loop based systems may not benefit at all from running on 64-core CPUs, unless 64 copies of them are configured to run simultaneously—which works great for web servers but not very often with databases.</p><p>Each of these ways of handling connections perform differently when used in application servers and databases, because of the distributed or centralized nature of each system. Application servers, for instance, tend to be horizontally scalable—they work correctly and in the same way whether you have 1 server or 10 or 10,000. In these cases, Moving away from the process / thread model tends to result in higher performance, because we want as much work to be done with minimal memory usage and CPU context switching. Event loops like Node work great on single core servers, but need to be clustered correctly to use multi-core severs. Coroutine / actor based systems like Go or Erlang will utilise every core of the CPU much easier because they&#x27;re designed to work that way, with many thousands of goroutines or actors running simultaneously on a single machine.</p><p><em>Centralized databases</em>, on the other hand, benefit more from process / thread / event loop handling, because based on the transactional guarantees of the system we don&#x27;t want multiple connections operating on the same data at the same time. The operations happening on multiple connections will have to lock during the transaction-sensitive parts of their work, or use other strategies like <a href="https://www.postgresql.org/docs/current/mvcc-intro.html">MVCC</a>, and the fewer possible connection handlers there are the better. These systems support a few connections on a single machine. On large server, PostgreSQL might manage a few hundred connections, MySQL might handle a couple thousand. Redis can handle the highest number of connections (maybe tens of thousands) because it manages to keep data consistent using an event loop—but this means that only one operation can happen at a time, so it&#x27;s not a free lunch.</p><p><em>Distributed databases</em> can and will attempt to move away from the process and thread based model: because they&#x27;re spreading data across multiple machines, they usually give up locking, embrace partitioning, and design for high connection volume across a large number of servers. AWS DynamoDB or Google Datastore for instance, or many distributed databases written in Go, will happily accept millions or billions of simultaneous connections. These decisions have consequences, though—they sacrifice a lot of the operations (joins, ad-hoc queries) and consistency guarantees provided by centralized / single-server databases. In return for this sacrifice, they can handle connections in a partitioned, horizontally scalable, practically unlimited way, allowing them to choose a design that supports many connections across many machines. This makes connections a non-issue here—each individual sever must worry about its connection handling, but in aggregate, across thousands or millions of machines with smart connection routing, these systems often behave like they&#x27;re infinitely scalable.</p><h2>[<!-- -->What&#x27;s | Why<!-- -->]<!-- --> a Pool?</h2><p>We need to be efficient and frugal with expensive connections. It often isn&#x27;t easy to realize how expensive they are because of the asymmetry: from the client&#x27;s point of view they look cheap—it&#x27;s usually the server that has a problem with too many connections.</p><p>When a client performs work, it does not have the luxury of being able to use one connection for one piece of work. An application server, for instance, accesses a database as a client over a connection. If it were to initiate a new connection for each request, it becomes artificially limited in its own capacity by the connection capacity of the database that it&#x27;s connecting to. There are a few cases where this usage is perfectly efficient—that might happen if the application server is purely a proxy for the database. In reality, the application servers are doing many other things: waiting for the request data to come in to the server, parsing the request, formulating a query, sending the query over the database connection, waiting for the results, parsing the results from the database connection protocol, reading or manipulating the result data, transforming the output into HTML/JSON/RPC formats, making network requests other services and more. The connection remains idle for most of this time, which means an expensive resource is not being used efficiently. And all this doesn&#x27;t even take into consideration the setup (process start, authentication) and shutdown costs of the connection on the server side.</p><p>To increase the efficiency of connection usages, many database clients will use what&#x27;s called a <em>connection pool</em>. A <em>pool</em> is an object that maintains a set of connections internally, without allowing direct access or usage. These connections are given out by the pool when communication with the database needs to happen, and returned to the pool when the communication is finished. The pool may be initialized with a configured number of connections, or it may be filled up lazily on-demand. The ideal usage of a connection pool would be that code requests a connection from the pool (called a <em>checkout</em>) just when it needs to use it, uses it, and puts it back in the pool (a <em>release</em>) immediately. This way, the code is not holding on to the connection while all the other non-connection related work is being done, greatly increasing efficiency. This allows many pieces of work to be done using one or a few connections. If all the connections in a pool are in use when a new <em>checkout</em> is requested, the requester will usually be made to wait (will <em>block</em>) until a connection has been <em>released</em>.</p><p>Different languages and frameworks will handle pooling differently. Ruby on Rails, for instance, handles checking out and release connections back into the pool automatically—but not understanding this leads to inefficient code. If you make a database request, followed by a long network request to another service, and another database request, the connection would have been held idle while the network request was happening (Rails auto-management needs to be conservative and cautious, therefore inefficient). Go has a standard library database driver that automatically uses a connection pool, but not realizing that connections are being released into the pool between database calls leads to surprising and hard-to-reproduce bugs. Developers sometimes assume that sequential operations in the same request will run on the same connection, but the automatic manager will swap connections out under you (Postgres <a href="https://www.postgresql.org/docs/current/explicit-locking.html#ADVISORY-LOCKS">advisory</a> locks <a href="https://engineering.qubecinema.com/2019/08/26/unlocking-advisory-locks.html">heisenbug spectacularly</a> in Go).</p><p>Transactions compound this problem: databases often base transaction functionality on a connection (sometimes called a <em>session</em>). If you start a transaction, you can commit it or roll it back only on the same connection that you started it on. Automatic pool management needs to be careful not to release connections while a transaction is in progress. Depending on the database, other functions like locks and prepared statements might also have connection affinity.</p><p>So if we want to write efficient code,  we need to know how connection pooling is happening in our framework, how much management is happening automatically, and when this management doesn&#x27;t work or is counter-productive. One common tool that helps take this off your mind is a <em>pooling proxy</em>, like <a href="https://pgdash.io/blog/pgbouncer-connection-pool.html">pgBouncer</a>, <a href="https://github.com/yandex/odyssey">Odyssey</a> or <a href="https://aws.amazon.com/rds/proxy/">AWS RDS Proxy</a>. These systems allow you to make as many database connections as you want without worrying about management, because the connections they give you are fake &amp; cheap simulated connections that they handle with low overhead. When you attempt to use one of these simulated connections, they pull out a real connection from an internal pool and map your fake connection on to the real one. After the proxy sees that you&#x27;ve finished using the connection, it keeps your fake connection open, but aggressively releases and re-uses the real connection. The connection counts and release-aggressiveness settings are configurable and help you tune for the gotchas like transactions, prepared statements, and locks.</p><p>Whether you choose to understand and manage connections efficiently in your code, or use a tool like pgBouncer, is ultimately a productivity and deployment complexity choice. Either option can work based on how much code you want to write, how easy connection management is in your chosen language, and how much efficiency you need in your project.</p><p>Pooling isn&#x27;t limited to database clients. We&#x27;ve been referring to connections as cheap on the client side, but they are not zero cost. They do use memory and ports and file descriptors on the client, which are not infinite resources. For this reason many languages / libraries will have a connection pool for HTTP connections made to the same server, and will also use pooling for other scarce resources. These pools tend to be hidden out of sight until a system runs out of the scarce resource, at which point it usually crashes. Knowing this helps a lot with debugging problems—besides connections, the other <a href="https://www.networkworld.com/article/2693414/setting-limits-with-ulimit.html">usual suspects</a> are file descriptors.  </p><h2>Configuring Common Pools</h2><p>Now that we&#x27;ve seen how connections are commonly handled, we can talk about a few different application server + database combinations, and reason about how to maximize the number of requests we can handle on the application server while minimizing the database connection count. While not every combination is covered here, most systems will have the same characteristics as one of the examples we cover—so knowing how these systems work will help you understand your own system even if it&#x27;s not covered here. If you want me to add more combinations or systems, do <a href="https://twitter.com/sudhirj">let me know</a>.</p><h3>Process &amp; Thread Based Handling</h3><p><a href="https://puma.io/">Puma</a>, a popular application server, runs Ruby applications with two kinds of pooling for the handlers of incoming HTTP requests. The first lever is the number of processes to start, represented by the <code>workers</code> configuration directive. Each process of the server is distinct and loads the full application stack independently into memory—so if your application occupies <em>N</em> MB or RAM, you&#x27;ll need to make sure you have at least <code>workers * N</code> MB of RAM to run that many copies of it. There is a way to alleviate this: Ruby 2+ supports the <em>copy-on-write</em> feature, which allows multiple processes to start as a single process and fork into multiple processes without necessarily copying all the memory—a lot of common memory areas will be shared until they&#x27;re modified in some way. Activating copy-on-write using the <code>preload_app!</code> directive <em>might</em> help you use less memory than the full multiple of the application size and the number of workers—but don&#x27;t count on it too much without testing how much of an advantage it gives you under sustained load.</p><p>Purely process based servers like <a href="https://github.com/defunkt/unicorn">Unicorn</a> stop at this level of configuration, as do the <a href="https://www.fullstackpython.com/wsgi-servers.html">popular servers for Python</a>, PHP and other languages that use a global lock or assume single-threaded single-process execution. Each process can handle one request at a time, but this doesn&#x27;t guarantee full utilization—if the process is waiting on a database query or a network request to another service, it&#x27;s not going to pick up a new request, and the CPU core that it&#x27;s on remains idle. To counter this wastage, you might start more processes than you have CPU cores (which results in context switching costs) or use threads.</p><p>Which brings us to the second lever Puma gives you—the number of <em>threads</em> to run in each of the processes / <em>workers</em> you&#x27;ve configured. Using the <code>threads</code> directive allows you configure the minimum and maximum number of threads in each worker&#x27;s <a href="https://github.com/puma/puma#thread-pool">thread pool</a>. Using these two directives allows you to control the total number of threads that will act as simultaneous request handlers for you application—this is simply the number of workers multiplied by the number of threads.</p><p>A rule of thumb would be to place one worker for each CPU core you have available—assuming, of course, that you have enough memory to do so. This utilizes your RAM effectively, so you can plan how much RAM you need based on a few tests with this number. Now we want to fully utilize the CPU—and we do this by increasing the max thread count. You&#x27;ll remember that threads share memory with their process, so they don&#x27;t contribute much towards raising your RAM requirements—instead more threads will further utilize your CPU while allowing your to handle more requests simultaneously. This is helpful because when one thread is sleeping because it&#x27;s waiting on a database query or network request, another thread from the same process can quickly be switched into the CPU core to do its work. But remember that many threads will also cause contention on the process locks, so you will discover a limit to how many threads you can add while still seeing a meaningful increase in performance.</p><p>How does all this configuration affect the number of database connections, though? Rails uses automatic connection management on the database, so each thread that you run will need its own database connection to function effectively, without waiting on others. It maintains a connection pool configured in the <code>database.yml</code> file, and this configuration applies at a process / worker level. So if the default value of <code>5</code> is left as-is, Rails will maintain a maximum of 5 connections per worker. This won&#x27;t work very well if you&#x27;re changing the max thread count—your many threads will all fight over the 5 connections in the pool. A rule of thumb would be to configure the <code>pool</code> count to be equal to the max <code>threads</code> count, as noted in the <a href="https://devcenter.heroku.com/articles/concurrency-and-database-connections#connection-pool">Heroku Puma deployment guide</a>.</p><p>Now this raises a new problem—having <code>workers * threads</code> database connections is great for application server performance, but will wreak havoc on a database like PostgreSQL and sometimes on MySQL. Depending on how much RAM (in the case of Postgres) and how much CPU (in the case of MySQL) you have, this configuration might not work for you. You might decide to reduce the <code>pool</code> or max <code>threads</code> value to make sure you have fewer connections, or the <code>workers</code> value, or both. Depending on your application, all are likely to have the same effect—if every request requires database queries, the number of database connections you have is the ultimate bottleneck for the number of requests you can handle. But if some requests can function without database access, you can get away with keeping the <code>workers</code> and <code>threads</code> count high, and the <code>pool</code> number relatively low—that way many threads will service your requests, but only a smaller subset will actually fight over the database connections in the pool as and when they need them. You can also to this if you take over connection management in your code and make sure to checkout and release connections efficiently—this is especially important if you&#x27;re making network requests between database calls.</p><p>If you find that it&#x27;s difficult to properly mange connections or tune these numbers, and your application server is being artificially limited by the database connection limits, you can then reach for a tool like pgBouncer, Odyssey, AWS RDS Proxy. Setting up a pooling proxy will allow you to set your pool size to be equal to the max thread count, and be confident that the proxy will make things efficient for you.</p><p>When it comes to databases, PostgreSQL <a href="https://www.citusdata.com/blog/2017/05/10/scaling-connections-in-postgres/">uses process based handlers</a>, so you&#x27;d want to be relatively stringent about connection counts when using it. MySQL <a href="https://mysqlserverteam.com/mysql-connection-handling-and-scaling/">uses threads</a>, so you could get away with making more connections to it—although this can result in performance drops because of context switching and locking.</p><h3>Event-Loop Based Handling</h3><p>Node / Deno is the first event-loop based server we&#x27;re looking at. This implies that starting a server with the default configurations will very effectively use the one CPU core that it&#x27;s running on, but will basically ignore the others. The internal subsystems and libraries may use the other cores, but for now we&#x27;re more interested in directly using them—and the way we do this is <em>clustering</em>. Clustering is achieved by starting one process that accepts all incoming connections, which then acts as proxy and distributes the connections over other processes running on the same machine. Node has a standard library <a href="https://nodejs.org/api/cluster.html">clustering module</a>, and popular servers like <a href="https://pm2.keymetrics.io/docs/usage/cluster-mode/">PM2</a> will use it effectively for you. The rule of thumb would be to run as many processes as there are CPU cores available, assuming there&#x27;s enough memory, of course.</p><p>Stripe also published the interesting <a href="https://github.com/stripe/einhorn">Einhorn project</a>, which is a connection manager that exists outside of the stack you&#x27;re writing code in. It starts its own process that accepts connections, and distributes them to instances of your application that it starts and manages as child processes. A tool like this is very useful in even loop based systems, which will make sure they fully utilize one CPU core given the chance—but it&#x27;s not as useful by itself with Ruby/Python, because while it would allow you to have multiple processes, the lack of threads mean that each processes would only be able to service one request at a time.</p><p>The clustered event loop approach is also used by systems that modify the default behaviour of a normally process based language. The <a href="https://www.tornadoweb.org/en/stable/">Tornado</a> server for Python, for example, converts Python request handling into an event-loop based system, also referred to as <em>non-blocking I/O</em>. It can also be <a href="https://www.tornadoweb.org/en/stable/guide/running.html#processes-and-ports">configured</a> to cluster itself on all available CPU cores, assuming enough memory.</p><p>A similar approach is also used in the <a href="https://socketry.github.io/falcon/index.html">Falcon</a> web-server for Ruby. Newer versions of Ruby a kind of <em>green-thread</em> called <em><a href="https://ruby-doc.org/core-3.0.0/Fiber.html">Fibers</a></em>, and Falcon handles each incoming request with one Ruby <em>Fiber</em>. Fibers do not automatically spread themselves across all CPU cores, so Falcon <a href="https://github.com/socketry/async-container">starts a copy</a> of your application in every available CPU core—again assuming there&#x27;s enough memory.</p><p>In all these cases, you&#x27;d want to configure your connection pools in your database adapters to limit the number of connections each process can make—or you might use a pooling proxy if your application servers are being limited by the database connection limits, or if managing connection checkout and release is getting difficult.</p><p>Redis <a href="https://redis.io/topics/clients">handles</a> connections with an event loop. This means that it can hold as many connections as its port, file descriptor, and memory resources allow—but it handles each operation from each connection one at a time.</p><h3>Internally Managed / Custom Handling</h3><p>The poster child for this kind of handling is <a href="https://golang.org/">Go</a>, which unlike all the other examples is completely unconstrained in the way it handles requests—it will unleash itself on your CPU and RAM with no restrictions. Every incoming request is handled by a new <em>goroutine</em>, a lightweight thread-like construct that the Go runtime internally manages and schedules much more efficiently than threads or processes. Go will also automatically spread its <em>goroutines</em> across all the CPU cores you have, although you can rein it in a bit with the <a href="https://golang.org/pkg/runtime/#GOMAXPROCS"><code>runtime.GOMAXPROCS</code></a> setting—and because this happens as part of the runtime, there is no memory copying happening. Go runs across all your CPU cores <em>without</em> having to start a new copy of your application on each of them.</p><p>Because Go is automatically geared for tens of thousands of simultaneous requests even on very small servers, pairing it with a process-based database like PosgreSQL is often like driving a race-car into a brick wall. If each goroutine is using the standard library <a href="https://golang.org/pkg/database/sql/">SQL</a> package, it will create as many connections as there are goroutines because the default pool size is unlimited. The first thing you&#x27;d want to do on any Go application using a SQL DB is to <a href="https://www.alexedwards.net/blog/configuring-sqldb">configure the connection limits</a> using <code>SetMaxOpenConns</code> and the associated options. Go uses an internal pool on the <code>sql</code> package, so each query you run will checkout a connection from the pool, use it and release it back immediately. This means that if you want to execute a transaction, you <em>must</em> use the <a href="https://golang.org/pkg/database/sql/#Conn.BeginTx">special methods</a> that give you an object that wraps the connection you started the transaction on, which is the only way you can later commit or rollback that transaction. This is also every important when using other functions that are connection specific, like prepared statements or advisory locks.</p><p>This automatic approach an unexpected benefit: it removes the need for pooling proxies, because the connections are already managed very aggressively. The database calls you make have efficient connection management by default, and when you do take over a connection it&#x27;s just as efficient as using a pooling proxy anyway. Systems that work this way default to doing the efficient thing, at the cost of you having to learn about how to handle the edge cases where you do want manual control. Besides the usual prepared statements and advisory locks gotchas, manually managing connections this also has potential deadlock problems. If a max connection count is configured and some requests try to checkout more than one connection simultaneously to do their work, there&#x27;s a chance they&#x27;ll get stuck forever waiting for each other to release connections. The <a href="https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing">pool sizing notes</a> on <a href="https://github.com/brettwooldridge/HikariCP">HikariCP</a> have formulae that help address problems like this.</p><p>Other VM based languages, like <a href="https://stackoverflow.com/questions/4436422/how-does-java-makes-use-of-multiple-cores">Java</a>, <a href="https://akka.io/">Scala/Akka</a>, Clojure, <a href="https://kotlinlang.org/docs/reference/coroutines-overview.html">Kotlin</a> (all on the JVM) and Elixir/Erlang (on the <a href="https://medium.com/flatiron-labs/elixir-and-the-beam-how-concurrency-really-works-3cc151cddd61">BEAM</a> VM) will function similarly, where using all available cores on a CPU will be possible without starting a new copy of the application on each one. Each specific system or database connection library will usually have slightly different implementations, but they should be understandable using one or more of the concepts described here.</p></article><footer class="py-10 text-gray-500 prose">Hi! I&#x27;m Sudhir Jonathan, I work as the Director of Engineering at <a href="https://www.qubewire.com">QubeWire</a>, and you can find me on <a href="https://twitter.com/sudhirj">Twitter</a>, <a href="https://github.com/sudhirj">Github</a>, and <a href="https://stackoverflow.com/users/73831/sudhir-jonathan">StackOverflow</a>. And on this blog, <a href="../index.html">sudhir.io</a>. Subscribe to the <a href="https://tinyletter.com/sudhir-io">mailing list</a> if you&#x27;d like to be notified when I post new articles.</footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/understanding-connections-pools","query":{},"buildId":"pkInIokvrXIdaP_t5UMJ1","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>